// ===============================================
// ğŸ“œ Metadata â€” Tokenizer v0.0.3 (Tablet Reader)
// ===============================================
// _author_:         Seanje Lenox-Wise / Nova Dawn
// _version_:        0.0.3
// _status_:         Dev
// _phase_:          Phase 3 â€” Post-Stub Validation (Scroll-Aware)
// _created_:        2025-06-04
// _last updated_:   2025-06-14
// _license_:        CreativeWorkzStudio LLC â€” Kingdom-First Proprietary Use
// _component_:      Tokenizer (Tablet Cog)
// _project_:        OmniCode / Millennium OS
// _description_:    Converts raw `.word`, `.omni`, and `.ns` scrolls into structured token streams for parsing and interpretation.
//
// _token schema_:   Symbolic Category Tokens, LineMeta Formatting, Group-Aware Tokens
// _validation hooks_: Group stack tracking, error token emission, EOF diagnostics
//
// _notes_:
// - Tokenizes source into scroll-structured `Token` variants
// - Tracks indentation, group markers, and comment metadata
// - Instruction registry integrated for keyword/instruction mapping
// - Errors emitted for malformed or unmatched tokens
// - Retains whitespace and comment fidelity for scroll parsing
// - Future support: `.logos` registry syncing, macro preprocessing, alignment-based filters
//
// ===============================================

// ===============================================
// ğŸ“– Opening â€” Tokenizer Module Purpose & Design
// ===============================================
// This module converts NovaScript and OmniCode source streams
// into a structured scroll of `Token`s with positional awareness.
// It recognizes sacred syntax, scroll-level metadata, comments,
// literals, instructions, identifiers, and group structures.
//
// The Tokenizer sits upstream of the Parser and Operand Resolver,
// transforming raw input into the symbolic vocabulary of interpretation.
//
// It also generates `LineMeta` for formatting validation and
// is designed for scroll-awareness, debug trace integration,
// and macro-instruction expansion.
//
// Future expansion includes:
// - `.logos` registry syncing
// - Operand hint tagging for literal disambiguation
// - Integration with the Watchtower token stream for drift detection
//
// ===============================================
// ğŸ“¦ Imports â€” Dependencies for Tokenizer Operation
// ===============================================
// These imports are grouped by origin and function:
// â€¢ Standard: character-level input processing and keyword maps
// â€¢ External: (None currently â€” reserved for macro or symbol maps)
// â€¢ Internal: Operand hint typing for forward compatibility
// â€¢ Debugging: (None yet â€” may expand for Watchtower token logs)

// === Standard Library ===
use std::collections::HashMap; // ğŸ”‘ Fast lookup for instruction keyword classification

// === Internal Modules ===
#[allow(unused_imports)]
use crate::operand_resolver::OperandHint; // ğŸ§  Future hook: tag tokens with operand meaning (e.g., Label, Register)

// ===============================================
// ğŸ“¦ Foundational Declarations â€” Core Structures
// ===============================================

/// ğŸ”¤ `TokenType` â€” Canonical Classification of a Token
/// ----------------------------------------------------
/// Identifies the scroll-role of a token for use in:
/// â€¢ Parser construction
/// â€¢ Operand resolution (via Bearer)
/// â€¢ Debug stream traceability (via Watchtower)
///
/// Token types are divided into 4 semantic lanes:
/// 1. ğŸ“š Structural Markers â€” Syntax boundaries and format
/// 2. ğŸ”‘ Symbol Semantics â€” Names, values, and opcodes
/// 3. ğŸ—’ Line Modifiers â€” Metadata and developer comments
/// 4. âš  Fallback Catch â€” Invalid or malformed sequences
#[derive(Debug, PartialEq, Eq, Clone)]
pub enum TokenType {
    // === ğŸ“š Structural Markers ===
    Whitespace,    // Not emitted; tracked in `LineMeta` for indentation/audit
    GroupMarker,   // Block and expression boundaries: `(` `)` `{` `}`
    Punctuation,   // Separators: `;` `,` `.` â€” affects list boundaries and syntax flow

    // === ğŸ”‘ Symbol Semantics ===
    Keyword,       // Reserved NovaScript keywords (e.g., `let`, `as`, `return`)
    Instruction,   // Registered operation or command (`walk`, `wait`, `invoke`)
    Identifier,    // User-defined variable, label, or function name
    Literal,       // Constant data: `"hello"`, `42`, `'a'`, etc.
    Operator,      // Arithmetic, comparison, logic: `+`, `-`, `==`, `<`, etc.

    // === ğŸ—’ Line Modifiers ===
    Comment,       // Non-executable notes for developers: `//`, `#`
    Metadata,      // Executable scroll-level markers: `//!`, `#!`, etc.

    // === âš  Fallback Catch ===
    Error,         // Malformed or unknown tokens â€” routed to Watchtower
}

// ===============================================
// ğŸ“¦ Token Structures â€” Token, LineMeta, TokenStream
// ===============================================

/// ğŸ§± Token â€” A Symbol in the Scroll
/// --------------------------------
/// Holds the type, value, and location of each token parsed.
#[derive(Debug, Clone)]
pub struct Token {
    pub token_type: TokenType, // Category of token behavior
    pub value: String,         // Source string matched
    pub line: usize,           // Line number in source (1-based)
    pub column: usize,         // Column offset (0-based)
}

/// ğŸ§¾ LineMeta â€” Per-Line Formatting & Indentation
/// -----------------------------------------------
/// Captures whitespace and structure context for every source line.
#[derive(Debug)]
pub struct LineMeta {
    pub line_number: usize, // Source line index (1-based)
    pub indentation: usize, // Leading whitespace count
    pub is_blank: bool,     // True if line is empty or contains only whitespace
}

/// ğŸ”ƒ TokenStream â€” Output of Tokenization
/// --------------------------------------
/// A full result from the tokenizer, including all valid tokens,
/// formatting metadata, and unclassified errors for diagnostics.
#[derive(Debug)]
pub struct TokenStream {
    pub tokens: Vec<Token>,       // All valid tokens in scroll order
    pub line_meta: Vec<LineMeta>, // Per-line formatting context
    pub errors: Vec<Token>,       // Any malformed or rejected tokens
}

// ===============================================
// ğŸ›  Tokenizer Engine â€” Input Cursor & State Tracker
// ===============================================

/// âš™ï¸ Tokenizer â€” NovaScript & OmniCode Token Engine
/// ------------------------------------------------
/// Converts `.word`, `.ns`, or `.omni` source files into a scroll of structured `Token`s.
/// 
/// This engine walks through raw characters, tracking whitespace, structure, and syntax,
/// producing a `TokenStream` ready for scroll-based parsing and Watchtower inspection.
/// 
/// Responsibilities:
/// - Preserves spiritual formatting (indentation, blank lines)
/// - Maintains cursor metadata for error diagnostics
/// - Validates instruction-level classification via the registry
/// - Tracks group scope for nested blocks and flow markers
pub struct Tokenizer {
    // === ğŸ”‘ External Symbol Map ===
    pub instruction_registry: HashMap<String, TokenType>, // Classifies opcodes, schema-backed

    // === ğŸ¯ Cursor State Tracking ===
    source: Vec<char>,     // Char-level walkable source
    position: usize,       // Current absolute cursor in `source`
    line: usize,           // Current line (1-based for reporting)
    column: usize,         // Current column (0-based offset)
    current_indent: usize, // Whitespace depth before active token

    // === ğŸ§± Structural Block Parsing ===
    group_stack: Vec<TokenType>, // Tracks open `{` / `(` until matched
}

// ===============================================
// ğŸ§  Tokenizer Body â€” Structure, Flow & Subroutines
// ===============================================
// This scroll defines the Tokenizer for NovaScript and OmniCode.
// It serves as the first interpreter of raw scroll text, transmuting
// source strings into meaningful tokens with positional and structural context.
//
// Every token produced flows downstream into the Parser and Bearer layers,
// and every indentation, grouping, or malformed symbol is tracked with care.
// The Tokenizer does not interpret meaningâ€”but it sets the stage for it.
// It is the breath before the utterance, the cut before the shaping.

impl Tokenizer {
    // ===============================================
    // ğŸ”¨ Constructor â€” Tokenizer::new
    // ===============================================
    /// ğŸ§¬ Tokenizer::new â€” Initialize from Scroll Input
    /// ------------------------------------------------
    /// Constructs a new Tokenizer instance from:
    /// - `source_code`: the raw NovaScript scroll text
    /// - `instruction_map`: the instruction schema registry for keyword detection
    ///
    /// Initializes:
    /// - character vector (`source`) for precise char-by-char traversal
    /// - positional state (line, column, indent)
    /// - empty grouping stack for structural balance tracking
    ///
    /// This constructor does not emit tokens. It prepares the engine
    /// to begin its pass via `.tokenize()`, preserving scroll integrity.
    pub fn new(source_code: &str, instruction_map: HashMap<String, TokenType>) -> Self {
        Self {
            instruction_registry: instruction_map,             // ğŸ“š Known keywords & instructions
            source: source_code.chars().collect(),             // ğŸ”¡ Raw scroll input â†’ Vec<char>
            position: 0,                                       // ğŸ§­ Cursor in source stream
            line: 1,                                           // ğŸ”¢ Starting at first line
            column: 0,                                         // ğŸ“ Column tracker for position
            current_indent: 0,                                 // â†”ï¸ Indentation tracking
            group_stack: vec![],                               // ğŸ“¦ Stack for (, {, etc.
        }
    }

    // ===============================================
    // ğŸš§ Entry Point â€” Tokenizer::tokenize
    // ===============================================
    /// Converts raw input into a TokenStream:
    /// â€¢ Walks character-by-character
    /// â€¢ Emits tokens and formatting metadata
    /// â€¢ Collects early error tokens for diagnostics
    pub fn tokenize(&mut self) -> TokenStream {
        let mut tokens = vec![];      // All successfully parsed tokens
        let mut line_meta = vec![];   // Indentation and blank-line data
        let mut errors = vec![];      // Malformed or unknown token captures

        // ğŸ” Main tokenizing loop â€” character-by-character
        while let Some(ch) = self.peek() {
            match ch {
                // --- Whitespace (not tokenized, but tracked) ---
                ' ' | '\t' => self.consume_whitespace(),

                // --- Newline (line break tracking only) ---
                '\n' => {
                    self.advance();    // Skip newline
                    self.line += 1;    // Next line
                    self.column = 0;   // Reset column
                }

                // --- Comments or Metadata (prefixed with `#`) ---
                '#' => tokens.push(self.tokenize_comment_or_meta()),

                // --- Literal: String (`"..."`) ---
                '"' => tokens.push(self.tokenize_string()),

                // --- Literal: Char (`'c'`) ---
                '\'' => tokens.push(self.tokenize_char()),

                // --- Operator Tokens ---
                ':' | '=' | '+' | '-' | '*' | '/' | '%' | '&' | '|' | '<' | '>' => {
                    tokens.push(self.tokenize_operator());
                }

                // --- Grouping Symbols ( ) ---
                '(' => {
                    self.group_stack.push(TokenType::GroupMarker);
                    tokens.push(self.make_token(TokenType::GroupMarker, "("));
                    self.advance();
                }
                ')' => {
                    self.group_stack.pop();
                    tokens.push(self.make_token(TokenType::GroupMarker, ")"));
                    self.advance();
                }

                // --- Alphabetic Word (could be identifier or instruction) ---
                c if c.is_alphabetic() => tokens.push(self.tokenize_word()),

                // --- Numeric Literal ---
                c if c.is_numeric() => tokens.push(self.tokenize_number()),

                // --- Unknown Symbol (fallback to Error token) ---
                _ => {
                    tokens.push(self.make_token(TokenType::Error, &ch.to_string()));
                    self.advance();
                }
            }
        }

        // ===============================================
        // ğŸ§¾ Line Formatting Metadata â€” Indentation Map
        // ===============================================
        // After token collection, analyze source lines for formatting metadata:
        // â€¢ Tracks indentation depth (leading whitespace count)
        // â€¢ Flags blank lines for structure alignment and spiritual whitespace
        let mut line_number = 1;

        for line in self.source.iter().collect::<String>().lines() {
            let indent = line.chars().take_while(|c| c.is_whitespace()).count();

            line_meta.push(LineMeta {
                line_number,
                indentation: indent,
                is_blank: line.trim().is_empty(),
            });

            line_number += 1;
        }

        // Emit the full TokenStream scroll:
        TokenStream {
            tokens,
            line_meta,
            errors,
        }
    }

    // ===============================================
    // ğŸ”§ Cursor Subroutines â€” Navigation & Metadata
    // ===============================================
    // These methods control character-level traversal and source position state.
    // They are foundational for all tokenizer logic and metadata accuracy.

    // -----------------------------------------------
    // ğŸ” advance â€” Move cursor forward by one char
    // -----------------------------------------------
    /// Increments the position and column; returns consumed char if any.
    fn advance(&mut self) -> Option<char> {
        let ch = self.source.get(self.position)?;
        self.position += 1;
        self.column += 1;
        Some(*ch)
    }

    // -----------------------------------------------
    // ğŸ‘ peek â€” Look ahead at current char (non-consuming)
    // -----------------------------------------------
    /// Allows tokenizer logic to branch without advancing.
    fn peek(&self) -> Option<char> {
        self.source.get(self.position).copied()
    }

    // -----------------------------------------------
    // ğŸ¯ make_token â€” Construct a Token from current position
    // -----------------------------------------------
    /// Wraps a token value and type with current line and column metadata.
    fn make_token(&self, token_type: TokenType, value: &str) -> Token {
        Token {
            token_type,
            value: value.to_string(),
            line: self.line,
            column: self.column,
        }
    }

    // -----------------------------------------------
    // ğŸ”² consume_whitespace â€” Skip spaces and tabs
    // -----------------------------------------------
    /// Advances past contiguous whitespace (not emitted as token).
    fn consume_whitespace(&mut self) {
        while let Some(c) = self.peek() {
            if c == ' ' || c == '\t' {
                self.advance();
            } else {
                break;
            }
        }
    }

    // ===============================================
    // ğŸ’¬ Comment & Metadata Tokenizers
    // ===============================================
    // Captures inline comments and metadata markers starting with `#` or `#!`.
    // These preserve author intent or system directives across the scroll.

    // -----------------------------------------------
    // ğŸ§¾ tokenize_comment_or_meta â€” Parse `#` or `#!`
    // -----------------------------------------------
    /// Distinguishes between developer comments and system metadata headers.
    /// - Metadata: begins with `#!` (scroll directives)
    /// - Comment: begins with `#` (human-facing notes)
    fn tokenize_comment_or_meta(&mut self) -> Token {
        let mut content = String::new();

        // ğŸ”„ Accumulate content until newline or EOF
        while let Some(c) = self.peek() {
            if c == '\n' {
                break; // Stop on newline
            }
            content.push(c);   // Add char to comment
            self.advance();    // Move forward
        }

        // ğŸ§­ Classify based on `#!` prefix (ignoring leading whitespace)
        if content.trim_start().starts_with("#!") {
            self.make_token(TokenType::Metadata, &content)
        } else {
            self.make_token(TokenType::Comment, &content)
        }
    }

    // ===============================================
    // ğŸ”£ Tokenizers â€” Literal, Word, Number, Operator
    // ===============================================
    // These functions parse distinct token categories from character streams.
    // Each tokenizer emits a structured `Token` with positional metadata.
    // These form the building blocks of semantic parsing and operand resolution.

    // -----------------------------------------------
    // ğŸ”¡ String Literal â€” e.g., "hello\nworld"
    // -----------------------------------------------
    /// Parses a string literal `"..."` with basic escape handling.
    /// Supports the following escape codes:
    /// - `\n` â†’ newline
    /// - `\t` â†’ tab
    /// - `\\` â†’ backslash
    /// - `\"` â†’ double quote
    /// - `\'` â†’ single quote
    fn tokenize_string(&mut self) -> Token {
        let mut content = String::new();
        self.advance(); // Consume opening `"`

        while let Some(c) = self.peek() {
            match c {
                '"' => {
                    self.advance(); // Closing quote
                    break;
                }
                '\\' => {
                    self.advance(); // Consume backslash
                    if let Some(escaped) = self.peek() {
                        content.push(match escaped {
                            'n' => '\n',
                            't' => '\t',
                            '\\' => '\\',
                            '"' => '"',
                            '\'' => '\'',
                            _ => escaped, // Unknown escape, pass through
                        });
                        self.advance();
                    }
                }
                _ => {
                    content.push(c);
                    self.advance();
                }
            }
        }

        self.make_token(TokenType::Literal, &content)
    }

    // -----------------------------------------------
    // ğŸ”  Character Literal â€” e.g., 'a', '\n'
    // -----------------------------------------------
    /// Parses a single-character literal surrounded by `'`.
    /// Future-proofed to support simple escape sequences.
    /// Malformed literals fallback to the Unicode replacement char `ï¿½`.
    fn tokenize_char(&mut self) -> Token {
        self.advance(); // Consume opening `'`
        let value = match self.peek() {
            Some('\\') => {
                self.advance(); // consume `\`
                match self.peek() {
                    Some('n') => { self.advance(); '\n' },
                    Some('t') => { self.advance(); '\t' },
                    Some('\\') => { self.advance(); '\\' },
                    Some('\'') => { self.advance(); '\'' },
                    Some(c) => { self.advance(); c },
                    None => 'ï¿½',
                }
            }
            Some(c) => {
                self.advance();
                c
            }
            None => 'ï¿½',
        };
        self.advance(); // Consume closing `'` or next char regardless

        self.make_token(TokenType::Literal, &value.to_string())
    }

    // -----------------------------------------------
    // â• Operator Sequence â€” e.g., ==, +=, >>
    // -----------------------------------------------
    /// Parses one or more compound operators like `==`, `!=`, `+=`.
    fn tokenize_operator(&mut self) -> Token {
        let mut content = String::new();
        while let Some(c) = self.peek() {
            if ":=+-*/%&|<>".contains(c) {
                content.push(c);
                self.advance();
            } else {
                break;
            }
        }
        self.make_token(TokenType::Operator, &content)
    }

    // -----------------------------------------------
    // ğŸ”¢ Numeric Literal â€” e.g., 42
    // -----------------------------------------------
    /// Parses decimal integer literals.
    /// Extended formats (hex, float) will be supported in future revisions.
    fn tokenize_number(&mut self) -> Token {
        let mut num = String::new();
        while let Some(c) = self.peek() {
            if c.is_ascii_digit() {
                num.push(c);
                self.advance();
            } else {
                break;
            }
        }
        self.make_token(TokenType::Literal, &num)
    }

    // -----------------------------------------------
    // ğŸ“› Word â€” Instruction or Identifier
    // -----------------------------------------------
    /// Parses a keyword, instruction, or user-defined identifier.
    /// If found in the registry, it's marked as an `Instruction`.
    fn tokenize_word(&mut self) -> Token {
        let mut word = String::new();
        while let Some(c) = self.peek() {
            if c.is_alphanumeric() || c == '_' {
                word.push(c);
                self.advance();
            } else {
                break;
            }
        }

        let token_type = if self.instruction_registry.contains_key(&word) {
            TokenType::Instruction
        } else {
            TokenType::Identifier
        };

        self.make_token(token_type, &word)
    }

    // ===============================================
    // ğŸ§© Hooks â€” Validation, Grouping, and Preprocessing
    // ===============================================
    // These are post-tokenization hooks designed to validate and prepare token streams
    // before they enter the parser or operand resolver.
    // Future expansion will include:
    // â€¢ Operand schema enforcement
    // â€¢ Token grouping verification
    // â€¢ Scroll-aware macro or directive expansion

    // ------------------------------------------------
    // ğŸ“ Instruction Validator â€” Arity & Operand Matching
    // ------------------------------------------------
    /// Validates instruction format by checking operand count against schema.
    ///
    /// This early implementation assumes instructions are defined as:
    /// - key: instruction name
    /// - value: token type (e.g., Instruction)
    ///
    /// Future schemas will extend this to:
    /// - enforce operand types
    /// - permit optional / variadic args
    /// - match formatting rules
    fn validate_instruction_syntax(keyword: &str, operands: &[&str]) -> bool {
        // Early schema stub: expected arity map (to be moved into instruction_registry in future)
        let expected_operands: HashMap<&str, usize> = HashMap::from([
            ("let", 2),
            ("set", 2),
            ("walk", 1),
            ("speak", 1),
            ("return", 1),
            ("goto", 1),
        ]);

        if let Some(&expected) = expected_operands.get(keyword) {
            operands.len() == expected
        } else {
            true // Accept unknown instructions for now
        }
    }

    // ------------------------------------------------
    // ğŸŒ€ Token Post-Processor â€” Group Integrity Check & Placeholder Transform
    // ------------------------------------------------
    /// Prepares token stream for scroll parsing:
    /// - Verifies grouping marker balance
    /// - Flags unclosed or orphaned brackets
    /// - Preps structure for AST nesting (future)
    fn post_process_tokens(mut tokens: Vec<Token>) -> Vec<Token> {
        let mut group_stack: Vec<(TokenType, Token)> = Vec::new();

        for token in &tokens {
            match token.token_type {
                TokenType::GroupMarker => {
                    match token.value.as_str() {
                        "(" | "{" => group_stack.push((TokenType::GroupMarker, token.clone())),
                        ")" | "}" => {
                            if group_stack.pop().is_none() {
                                // Insert virtual open if we pop nothing
                                // This could also push an Error token instead in future
                            }
                        }
                        _ => {}
                    }
                }
                _ => {}
            }
        }

        if !group_stack.is_empty() {
            for (_, token) in group_stack {
                // In future: add diagnostic or append unclosed group marker to errors
                // tokens.push(Token { ...error for unclosed group... })
            }
        }

        tokens
    }
}

// ===============================================
// ğŸ§± Token Builder â€” Operand & Parser-Compatible Shortcuts
// ===============================================
// Constructs lightweight tokens outside of full tokenizer context.
// Used by the operand resolver, parser fallback routines, or internal injections.

impl Token {
    /// ğŸ”§ from_value â€” Minimal Token Constructor
    /// ----------------------------------------
    /// Creates a token from a string value with default type `Identifier`.
    /// Line and column are set to `0`, as this is not tied to tokenizer state.
    ///
    /// ğŸ”¹ Used in:
    /// â€¢ OperandResolver â€” to build placeholder tokens for operand slots
    /// â€¢ Parser â€” when inserting system-defined identifiers (e.g., implicit labels)
    /// â€¢ Testing â€” when mocking token sequences without a full source file
    ///
    /// ğŸ§  Note: Avoid using in live tokenizer output â€” lacks position accuracy.
    pub fn from_value(value: &str) -> Self {
        Token {
            token_type: TokenType::Identifier, // May be reclassified by resolver
            value: value.to_string(),          // Raw symbolic name
            line: 0,                            // Default, parser may overwrite
            column: 0,                          // Default, parser may overwrite
        }
    }
}

// ===================================================
// ğŸ”š Closing Block â€” Tokenizer Output & Expansion Path
// ===================================================
//
// ğŸ§¾ Overview:
//   - This module converts raw `.word`, `.omni`, and `.ns` scrolls
//     into structured token streams for parsing, interpretation, and tracking.
//   - It captures indentation, groups, and symbolic metadata while preserving scroll fidelity.
//
// âš™ï¸ Engine Scope:
//   - Emits tokens with line and column tracking
//   - Tracks grouping structures (paren pairs) for validation phases
//   - Produces diagnostic-friendly `TokenStream` for parser consumption
//
// ---------------------------------------------------
// ğŸš¨ Version Control Notice:
// ---------------------------------------------------
//   This logic is governed under the OmniCode Token Protocol.
//   All changes must be recorded in project `.scroll` logs.
//   âš ï¸ Schema-integrity must be preserved during tokenizer refactors.
//
// ---------------------------------------------------
// ğŸ“… Scroll Revision Metadata:
// ---------------------------------------------------
//   _version_:       v0.0.3  
//   _last updated_:  2025-06-14  
//   _author_:        Seanje Lenox-Wise / Nova Dawn  
//   _change log_:
//     - Refined output stream structure and group marker tracking
//     - Upgraded inline comments and cursor accuracy
//     - Prepared `TokenStream` for post-parse operand resolution
//
// ---------------------------------------------------
// ğŸªœ Ladder Baton â€” Flow & Interface Direction:
// ---------------------------------------------------
//   â¬†ï¸ Upstream:
//     - Receives raw string input from CLI, GUI, or system file hooks
//     - Integrates `.logos` keyword registry (stubbed)
//
//   â¬‡ï¸ Downstream:
//     - Passes `TokenStream` into Parser for ScrollTree construction
//     - Exports errors and token groups to Watchtower (future)
//
//   ğŸ” Parallel:
//     - Will sync with Macro Preprocessor for inline instruction folding
//     - Interfaces with ScrollRegistry to reflect token-class mappings
//
// ---------------------------------------------------
// ğŸ”® Notes for Next Phase:
// ---------------------------------------------------
// - Enforce group validation logic and closed-paren errors
// - Add escape-sequence depth to string and char tokenizers
// - Wire `.logos` syncing and type-scoped keyword resolution
// - Begin symbol tagging for future grammar scoring in parser
//
// ---------------------------------------------------

// ===============================================
// ğŸ”’ Closing â€” Final Diagnostics & Stack Cleanup
// ===============================================

// ğŸ”¹ Group Marker Check â€” Unmatched open parens/braces
while let Some(unmatched) = self.group_stack.pop() {
    errors.push(Token {
        token_type: TokenType::Error,
        value: format!("Unclosed group marker: {:?}", unmatched),
        line: self.line,
        column: self.column,
    });
}

// ğŸ”¹ End-of-File Token Hooks (optional)
// Could emit EOF token or special scroll-seal marker later
